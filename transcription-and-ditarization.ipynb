{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCmjcOc9yEtQ"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn1c-CoDv2kw",
        "outputId": "4845f1a3-937f-490d-f6fa-dd3f9d599360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting git+https://github.com/m-bain/whisperX.git@78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Cloning https://github.com/m-bain/whisperX.git (to revision 78dcfaab51005aa703ee21375f81ed31bc248560) to /tmp/pip-req-build-q79q2sre\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperX.git /tmp/pip-req-build-q79q2sre\n",
            "  Running command git rev-parse -q --verify 'sha^78dcfaab51005aa703ee21375f81ed31bc248560'\n",
            "  Running command git fetch -q https://github.com/m-bain/whisperX.git 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Running command git checkout -q 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Resolved https://github.com/m-bain/whisperX.git to commit 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting torch>=2\n",
            "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Collecting torchaudio>=2\n",
            "  Using cached torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "Collecting faster-whisper==1.0.0\n",
            "  Using cached faster_whisper-1.0.0-py3-none-any.whl (1.5 MB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Collecting setuptools>=65\n",
            "  Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting pyannote.audio==3.1.1\n",
            "  Using cached pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n",
            "Collecting av==11.*\n",
            "  Using cached av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "Collecting tokenizers<0.16,>=0.13\n",
            "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Collecting ctranslate2<5,>=4.0\n",
            "  Using cached ctranslate2-4.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (179.4 MB)\n",
            "Collecting onnxruntime<2,>=1.14\n",
            "  Using cached onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "Collecting huggingface-hub>=0.13\n",
            "  Using cached huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "Collecting speechbrain>=0.5.14\n",
            "  Using cached speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
            "Collecting torch-audiomentations>=0.11.0\n",
            "  Using cached torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
            "Collecting omegaconf<3.0,>=2.1\n",
            "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Collecting torchmetrics>=0.11.0\n",
            "  Using cached torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "Collecting pyannote.metrics>=3.2\n",
            "  Using cached pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "Collecting pyannote.database>=5.0.1\n",
            "  Using cached pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
            "Collecting pyannote.pipeline>=3.0.1\n",
            "  Using cached pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Collecting asteroid-filterbanks>=0.4\n",
            "  Using cached asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting pyannote.core>=5.0.0\n",
            "  Using cached pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "Collecting lightning>=2.0.1\n",
            "  Using cached lightning-2.2.4-py3-none-any.whl (2.0 MB)\n",
            "Collecting soundfile>=0.12.1\n",
            "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "Collecting semver>=3.0.0\n",
            "  Using cached semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Collecting tensorboardX>=2.6\n",
            "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Collecting einops>=0.6.0\n",
            "  Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "Collecting pytorch-metric-learning>=2.1.0\n",
            "  Using cached pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n",
            "Collecting rich>=12.0.0\n",
            "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting typing-extensions>=4.8.0\n",
            "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Collecting triton==2.3.0\n",
            "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "Collecting jinja2\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-nvjitlink-cu12\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting click\n",
            "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Collecting numpy>=1.22.4\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting python-dateutil>=2.8.2\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Collecting tzdata>=2022.7\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Collecting safetensors>=0.4.1\n",
            "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "  Using cached transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0\n",
            "  Using cached lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Collecting pytorch-lightning\n",
            "  Using cached pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
            "Collecting coloredlogs\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers\n",
            "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Collecting sortedcontainers>=2.0.4\n",
            "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting scipy>=1.1\n",
            "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Collecting typer>=0.12.1\n",
            "  Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "Collecting matplotlib>=2.0.0\n",
            "  Using cached matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "Collecting docopt>=0.6.2\n",
            "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
            "Collecting scikit-learn>=0.17.1\n",
            "  Using cached scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "Collecting tabulate>=0.7.7\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting optuna>=3.1\n",
            "  Using cached optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "Collecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "Collecting cffi>=1.0\n",
            "  Using cached cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "Collecting hyperpyyaml\n",
            "  Using cached HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Collecting torch-pitch-shift>=1.2.2\n",
            "  Using cached torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting librosa>=0.6.0\n",
            "  Using cached librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Using cached julius-0.2.7-py3-none-any.whl\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 KB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting decorator>=4.3.0\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting soxr>=0.3.2\n",
            "  Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting lazy-loader>=0.1\n",
            "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Collecting msgpack>=1.0\n",
            "  Using cached msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
            "Collecting audioread>=2.1.9\n",
            "  Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Collecting pooch>=1.1\n",
            "  Using cached pooch-1.8.1-py3-none-any.whl (62 kB)\n",
            "Collecting numba>=0.51.0\n",
            "  Using cached numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "Collecting mdurl~=0.1\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting pillow>=8\n",
            "  Using cached pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Collecting kiwisolver>=1.3.1\n",
            "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Using cached fonttools-4.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "Collecting pyparsing>=2.3.1\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "Collecting colorlog\n",
            "  Using cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Collecting sqlalchemy>=1.3.0\n",
            "  Using cached SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting primePy>=1.3\n",
            "  Using cached primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting shellingham>=1.3.0\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Collecting ruamel.yaml>=0.17.28\n",
            "  Using cached ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "Collecting async-timeout<5.0,>=4.0\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting Mako\n",
            "  Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0\n",
            "  Using cached llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
            "Collecting platformdirs>=2.5.0\n",
            "  Using cached platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7\n",
            "  Using cached ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "Building wheels for collected packages: whisperx\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for whisperx: filename=whisperx-3.1.1-py3-none-any.whl size=38613 sha256=c03f5e71429a88e305be6af7708e1e20e0b7c700378ebfb5b641f28d6f724103\n",
            "  Stored in directory: /home/isa/.cache/pip/wheels/f9/d3/58/f5849c3e001ad6c42f42a1ca4fb67310016a235133d7e081ed\n",
            "Successfully built whisperx\n",
            "Installing collected packages: sortedcontainers, sentencepiece, pytz, primePy, mpmath, flatbuffers, docopt, antlr4-python3-runtime, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, tabulate, sympy, six, shellingham, setuptools, semver, safetensors, ruamel.yaml.clib, regex, pyyaml, pyparsing, pygments, pycparser, protobuf, platformdirs, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgpack, mdurl, MarkupSafe, llvmlite, kiwisolver, joblib, idna, humanfriendly, greenlet, fsspec, frozenlist, fonttools, filelock, einops, decorator, cycler, colorlog, click, charset-normalizer, certifi, av, audioread, attrs, async-timeout, yarl, triton, tensorboardX, sqlalchemy, soxr, scipy, ruamel.yaml, requests, python-dateutil, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, markdown-it-py, Mako, lightning-utilities, lazy-loader, jinja2, ctranslate2, contourpy, coloredlogs, cffi, aiosignal, soundfile, scikit-learn, rich, pyannote.core, pooch, pandas, onnxruntime, nvidia-cusolver-cu12, matplotlib, hyperpyyaml, huggingface-hub, alembic, aiohttp, typer, torch, tokenizers, optuna, librosa, transformers, torchmetrics, torchaudio, pytorch-metric-learning, pyannote.database, julius, faster-whisper, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio, whisperx\n",
            "  Attempting uninstall: sortedcontainers\n",
            "    Found existing installation: sortedcontainers 2.4.0\n",
            "    Uninstalling sortedcontainers-2.4.0:\n",
            "      Successfully uninstalled sortedcontainers-2.4.0\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.1\n",
            "    Uninstalling pytz-2024.1:\n",
            "      Successfully uninstalled pytz-2024.1\n",
            "  Attempting uninstall: primePy\n",
            "    Found existing installation: primePy 1.3\n",
            "    Uninstalling primePy-1.3:\n",
            "      Successfully uninstalled primePy-1.3\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: docopt\n",
            "    Found existing installation: docopt 0.6.2\n",
            "    Uninstalling docopt-0.6.2:\n",
            "      Successfully uninstalled docopt-0.6.2\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.1\n",
            "    Uninstalling urllib3-2.2.1:\n",
            "      Successfully uninstalled urllib3-2.2.1\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2024.1\n",
            "    Uninstalling tzdata-2024.1:\n",
            "      Successfully uninstalled tzdata-2024.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.9.0\n",
            "    Uninstalling tabulate-0.9.0:\n",
            "      Successfully uninstalled tabulate-0.9.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: shellingham\n",
            "    Found existing installation: shellingham 1.5.4\n",
            "    Uninstalling shellingham-1.5.4:\n",
            "      Successfully uninstalled shellingham-1.5.4\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 69.5.1\n",
            "    Uninstalling setuptools-69.5.1:\n",
            "      Successfully uninstalled setuptools-69.5.1\n",
            "  Attempting uninstall: semver\n",
            "    Found existing installation: semver 3.0.2\n",
            "    Uninstalling semver-3.0.2:\n",
            "      Successfully uninstalled semver-3.0.2\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.3\n",
            "    Uninstalling safetensors-0.4.3:\n",
            "      Successfully uninstalled safetensors-0.4.3\n",
            "  Attempting uninstall: ruamel.yaml.clib\n",
            "    Found existing installation: ruamel.yaml.clib 0.2.8\n",
            "    Uninstalling ruamel.yaml.clib-0.2.8:\n",
            "      Successfully uninstalled ruamel.yaml.clib-0.2.8\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.10\n",
            "    Uninstalling regex-2024.5.10:\n",
            "      Successfully uninstalled regex-2024.5.10\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.18.0\n",
            "    Uninstalling Pygments-2.18.0:\n",
            "      Successfully uninstalled Pygments-2.18.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.22\n",
            "    Uninstalling pycparser-2.22:\n",
            "      Successfully uninstalled pycparser-2.22\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.2.2\n",
            "    Uninstalling platformdirs-4.2.2:\n",
            "      Successfully uninstalled platformdirs-4.2.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.3.0\n",
            "    Uninstalling pillow-10.3.0:\n",
            "      Successfully uninstalled pillow-10.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.0.5\n",
            "    Uninstalling multidict-6.0.5:\n",
            "      Successfully uninstalled multidict-6.0.5\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.8\n",
            "    Uninstalling msgpack-1.0.8:\n",
            "      Successfully uninstalled msgpack-1.0.8\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.42.0\n",
            "    Uninstalling llvmlite-0.42.0:\n",
            "      Successfully uninstalled llvmlite-0.42.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.5\n",
            "    Uninstalling kiwisolver-1.4.5:\n",
            "      Successfully uninstalled kiwisolver-1.4.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: humanfriendly\n",
            "    Found existing installation: humanfriendly 10.0\n",
            "    Uninstalling humanfriendly-10.0:\n",
            "      Successfully uninstalled humanfriendly-10.0\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.0.3\n",
            "    Uninstalling greenlet-3.0.3:\n",
            "      Successfully uninstalled greenlet-3.0.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.3.1\n",
            "    Uninstalling fsspec-2024.3.1:\n",
            "      Successfully uninstalled fsspec-2024.3.1\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.4.1\n",
            "    Uninstalling frozenlist-1.4.1:\n",
            "      Successfully uninstalled frozenlist-1.4.1\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.51.0\n",
            "    Uninstalling fonttools-4.51.0:\n",
            "      Successfully uninstalled fonttools-4.51.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.14.0\n",
            "    Uninstalling filelock-3.14.0:\n",
            "      Successfully uninstalled filelock-3.14.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: colorlog\n",
            "    Found existing installation: colorlog 6.8.2\n",
            "    Uninstalling colorlog-6.8.2:\n",
            "      Successfully uninstalled colorlog-6.8.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: av\n",
            "    Found existing installation: av 11.0.0\n",
            "    Uninstalling av-11.0.0:\n",
            "      Successfully uninstalled av-11.0.0\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 3.0.1\n",
            "    Uninstalling audioread-3.0.1:\n",
            "      Successfully uninstalled audioread-3.0.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.3\n",
            "    Uninstalling async-timeout-4.0.3:\n",
            "      Successfully uninstalled async-timeout-4.0.3\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.9.4\n",
            "    Uninstalling yarl-1.9.4:\n",
            "      Successfully uninstalled yarl-1.9.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: tensorboardX\n",
            "    Found existing installation: tensorboardX 2.6.2.2\n",
            "    Uninstalling tensorboardX-2.6.2.2:\n",
            "      Successfully uninstalled tensorboardX-2.6.2.2\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.30\n",
            "    Uninstalling SQLAlchemy-2.0.30:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.30\n",
            "  Attempting uninstall: soxr\n",
            "    Found existing installation: soxr 0.3.7\n",
            "    Uninstalling soxr-0.3.7:\n",
            "      Successfully uninstalled soxr-0.3.7\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.0\n",
            "    Uninstalling scipy-1.13.0:\n",
            "      Successfully uninstalled scipy-1.13.0\n",
            "  Attempting uninstall: ruamel.yaml\n",
            "    Found existing installation: ruamel.yaml 0.18.6\n",
            "    Uninstalling ruamel.yaml-0.18.6:\n",
            "      Successfully uninstalled ruamel.yaml-0.18.6\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.59.1\n",
            "    Uninstalling numba-0.59.1:\n",
            "      Successfully uninstalled numba-0.59.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: Mako\n",
            "    Found existing installation: Mako 1.3.5\n",
            "    Uninstalling Mako-1.3.5:\n",
            "      Successfully uninstalled Mako-1.3.5\n",
            "  Attempting uninstall: lightning-utilities\n",
            "    Found existing installation: lightning-utilities 0.11.2\n",
            "    Uninstalling lightning-utilities-0.11.2:\n",
            "      Successfully uninstalled lightning-utilities-0.11.2\n",
            "  Attempting uninstall: lazy-loader\n",
            "    Found existing installation: lazy_loader 0.4\n",
            "    Uninstalling lazy_loader-0.4:\n",
            "      Successfully uninstalled lazy_loader-0.4\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: ctranslate2\n",
            "    Found existing installation: ctranslate2 4.2.1\n",
            "    Uninstalling ctranslate2-4.2.1:\n",
            "      Successfully uninstalled ctranslate2-4.2.1\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.1\n",
            "    Uninstalling contourpy-1.2.1:\n",
            "      Successfully uninstalled contourpy-1.2.1\n",
            "  Attempting uninstall: coloredlogs\n",
            "    Found existing installation: coloredlogs 15.0.1\n",
            "    Uninstalling coloredlogs-15.0.1:\n",
            "      Successfully uninstalled coloredlogs-15.0.1\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.16.0\n",
            "    Uninstalling cffi-1.16.0:\n",
            "      Successfully uninstalled cffi-1.16.0\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.1\n",
            "    Uninstalling aiosignal-1.3.1:\n",
            "      Successfully uninstalled aiosignal-1.3.1\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.12.1\n",
            "    Uninstalling soundfile-0.12.1:\n",
            "      Successfully uninstalled soundfile-0.12.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.4.2\n",
            "    Uninstalling scikit-learn-1.4.2:\n",
            "      Successfully uninstalled scikit-learn-1.4.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.1\n",
            "    Uninstalling rich-13.7.1:\n",
            "      Successfully uninstalled rich-13.7.1\n",
            "  Attempting uninstall: pyannote.core\n",
            "    Found existing installation: pyannote.core 5.0.0\n",
            "    Uninstalling pyannote.core-5.0.0:\n",
            "      Successfully uninstalled pyannote.core-5.0.0\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.8.1\n",
            "    Uninstalling pooch-1.8.1:\n",
            "      Successfully uninstalled pooch-1.8.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: onnxruntime\n",
            "    Found existing installation: onnxruntime 1.17.3\n",
            "    Uninstalling onnxruntime-1.17.3:\n",
            "      Successfully uninstalled onnxruntime-1.17.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.9.0\n",
            "    Uninstalling matplotlib-3.9.0:\n",
            "      Successfully uninstalled matplotlib-3.9.0\n",
            "  Attempting uninstall: hyperpyyaml\n",
            "    Found existing installation: HyperPyYAML 1.2.2\n",
            "    Uninstalling HyperPyYAML-1.2.2:\n",
            "      Successfully uninstalled HyperPyYAML-1.2.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.0\n",
            "    Uninstalling huggingface-hub-0.23.0:\n",
            "      Successfully uninstalled huggingface-hub-0.23.0\n",
            "  Attempting uninstall: alembic\n",
            "    Found existing installation: alembic 1.13.1\n",
            "    Uninstalling alembic-1.13.1:\n",
            "      Successfully uninstalled alembic-1.13.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.5\n",
            "    Uninstalling aiohttp-3.9.5:\n",
            "      Successfully uninstalled aiohttp-3.9.5\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.12.3\n",
            "    Uninstalling typer-0.12.3:\n",
            "      Successfully uninstalled typer-0.12.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0\n",
            "    Uninstalling torch-2.3.0:\n",
            "      Successfully uninstalled torch-2.3.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: optuna\n",
            "    Found existing installation: optuna 3.6.1\n",
            "    Uninstalling optuna-3.6.1:\n",
            "      Successfully uninstalled optuna-3.6.1\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 1.4.0.post0\n",
            "    Uninstalling torchmetrics-1.4.0.post0:\n",
            "      Successfully uninstalled torchmetrics-1.4.0.post0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.0\n",
            "    Uninstalling torchaudio-2.3.0:\n",
            "      Successfully uninstalled torchaudio-2.3.0\n",
            "  Attempting uninstall: pytorch-metric-learning\n",
            "    Found existing installation: pytorch-metric-learning 2.5.0\n",
            "    Uninstalling pytorch-metric-learning-2.5.0:\n",
            "      Successfully uninstalled pytorch-metric-learning-2.5.0\n",
            "  Attempting uninstall: pyannote.database\n",
            "    Found existing installation: pyannote.database 5.1.0\n",
            "    Uninstalling pyannote.database-5.1.0:\n",
            "      Successfully uninstalled pyannote.database-5.1.0\n",
            "  Attempting uninstall: julius\n",
            "    Found existing installation: julius 0.2.7\n",
            "    Uninstalling julius-0.2.7:\n",
            "      Successfully uninstalled julius-0.2.7\n",
            "  Attempting uninstall: faster-whisper\n",
            "    Found existing installation: faster-whisper 1.0.0\n",
            "    Uninstalling faster-whisper-1.0.0:\n",
            "      Successfully uninstalled faster-whisper-1.0.0\n",
            "  Attempting uninstall: asteroid-filterbanks\n",
            "    Found existing installation: asteroid-filterbanks 0.4.0\n",
            "    Uninstalling asteroid-filterbanks-0.4.0:\n",
            "      Successfully uninstalled asteroid-filterbanks-0.4.0\n",
            "  Attempting uninstall: torch-pitch-shift\n",
            "    Found existing installation: torch-pitch-shift 1.2.4\n",
            "    Uninstalling torch-pitch-shift-1.2.4:\n",
            "      Successfully uninstalled torch-pitch-shift-1.2.4\n",
            "  Attempting uninstall: speechbrain\n",
            "    Found existing installation: speechbrain 1.0.0\n",
            "    Uninstalling speechbrain-1.0.0:\n",
            "      Successfully uninstalled speechbrain-1.0.0\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.2.4\n",
            "    Uninstalling pytorch-lightning-2.2.4:\n",
            "      Successfully uninstalled pytorch-lightning-2.2.4\n",
            "  Attempting uninstall: pyannote.pipeline\n",
            "    Found existing installation: pyannote.pipeline 3.0.1\n",
            "    Uninstalling pyannote.pipeline-3.0.1:\n",
            "      Successfully uninstalled pyannote.pipeline-3.0.1\n",
            "  Attempting uninstall: pyannote.metrics\n",
            "    Found existing installation: pyannote.metrics 3.2.1\n",
            "    Uninstalling pyannote.metrics-3.2.1:\n",
            "      Successfully uninstalled pyannote.metrics-3.2.1\n",
            "  Attempting uninstall: torch-audiomentations\n",
            "    Found existing installation: torch-audiomentations 0.11.1\n",
            "    Uninstalling torch-audiomentations-0.11.1:\n",
            "      Successfully uninstalled torch-audiomentations-0.11.1\n",
            "  Attempting uninstall: lightning\n",
            "    Found existing installation: lightning 2.2.4\n",
            "    Uninstalling lightning-2.2.4:\n",
            "      Successfully uninstalled lightning-2.2.4\n",
            "  Attempting uninstall: pyannote.audio\n",
            "    Found existing installation: pyannote.audio 3.1.1\n",
            "    Uninstalling pyannote.audio-3.1.1:\n",
            "      Successfully uninstalled pyannote.audio-3.1.1\n",
            "  Attempting uninstall: whisperx\n",
            "    Found existing installation: whisperx 3.1.1\n",
            "    Uninstalling whisperx-3.1.1:\n",
            "      Successfully uninstalled whisperx-3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "wandb 0.17.0 requires protobuf!=4.21.0,<5,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 5.26.1 which is incompatible.\n",
            "demucs 4.1.0a2 requires torchaudio<2.1,>=0.8, but you have torchaudio 2.3.0 which is incompatible.\n",
            "datasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 MarkupSafe-2.1.5 aiohttp-3.9.5 aiosignal-1.3.1 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 async-timeout-4.0.3 attrs-23.2.0 audioread-3.0.1 av-11.0.0 certifi-2024.2.2 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 coloredlogs-15.0.1 colorlog-6.8.2 contourpy-1.2.1 ctranslate2-4.2.1 cycler-0.12.1 decorator-5.1.1 docopt-0.6.2 einops-0.8.0 faster-whisper-1.0.0 filelock-3.14.0 flatbuffers-24.3.25 fonttools-4.51.0 frozenlist-1.4.1 fsspec-2024.5.0 greenlet-3.0.3 huggingface-hub-0.23.0 humanfriendly-10.0 hyperpyyaml-1.2.2 idna-3.7 jinja2-3.1.4 joblib-1.4.2 julius-0.2.7 kiwisolver-1.4.5 lazy-loader-0.4 librosa-0.10.2.post1 lightning-2.2.4 lightning-utilities-0.11.2 llvmlite-0.42.0 markdown-it-py-3.0.0 matplotlib-3.9.0 mdurl-0.1.2 mpmath-1.3.0 msgpack-1.0.8 multidict-6.0.5 networkx-3.3 nltk-3.8.1 numba-0.59.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnxruntime-1.17.3 optuna-3.6.1 packaging-24.0 pandas-2.2.2 pillow-10.3.0 platformdirs-4.2.2 pooch-1.8.1 primePy-1.3 protobuf-5.26.1 pyannote.audio-3.1.1 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pycparser-2.22 pygments-2.18.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytorch-lightning-2.2.4 pytorch-metric-learning-2.5.0 pytz-2024.1 pyyaml-6.0.1 regex-2024.5.15 requests-2.31.0 rich-13.7.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 semver-3.0.2 sentencepiece-0.2.0 setuptools-69.5.1 shellingham-1.5.4 six-1.16.0 sortedcontainers-2.4.0 soundfile-0.12.1 soxr-0.3.7 speechbrain-1.0.0 sqlalchemy-2.0.30 sympy-1.12 tabulate-0.9.0 tensorboardX-2.6.2.2 threadpoolctl-3.5.0 tokenizers-0.15.2 torch-2.3.0 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchaudio-2.3.0 torchmetrics-1.4.0.post0 tqdm-4.66.4 transformers-4.39.3 triton-2.3.0 typer-0.12.3 typing-extensions-4.11.0 tzdata-2024.1 urllib3-2.2.1 whisperx-3.1.1 yarl-1.9.4\n",
            "zsh:1: no matches found: nemo_toolkit[asr]==1.23.0\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting demucs\n",
            "  Cloning https://github.com/facebookresearch/demucs to /tmp/pip-install-9hqfhg5y/demucs_6e4df225b2904f1c9670b85407829e0b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/demucs /tmp/pip-install-9hqfhg5y/demucs_6e4df225b2904f1c9670b85407829e0b\n",
            "  Resolved https://github.com/facebookresearch/demucs to commit e976d93ecc3865e5757426930257e200846a520a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: demucs\n",
            "  Building wheel for demucs (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for demucs: filename=demucs-4.1.0a2-py3-none-any.whl size=83549 sha256=17040fbdc7d3e1c7fcbba2eadffb946a89a57468865e2386e960309131ef459e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2hjejji6/wheels/45/e4/ca/7791f04b554e5433713e22900eaf11595e27c454fb65ac30ab\n",
            "Successfully built demucs\n",
            "Installing collected packages: demucs\n",
            "  Attempting uninstall: demucs\n",
            "    Found existing installation: demucs 4.1.0a2\n",
            "    Uninstalling demucs-4.1.0a2:\n",
            "      Successfully uninstalled demucs-4.1.0a2\n",
            "Successfully installed demucs-4.1.0a2\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting git+https://github.com/oliverguhr/deepmultilingualpunctuation.git\n",
            "  Cloning https://github.com/oliverguhr/deepmultilingualpunctuation.git to /tmp/pip-req-build-4lwups23\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/oliverguhr/deepmultilingualpunctuation.git /tmp/pip-req-build-4lwups23\n",
            "  Resolved https://github.com/oliverguhr/deepmultilingualpunctuation.git to commit 5a0dd7f4fd56687f59405aa8eba1144393d8b74b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers\n",
            "  Using cached transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "Collecting torch>=1.8.1\n",
            "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting triton==2.3.0\n",
            "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting typing-extensions>=4.8.0\n",
            "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting jinja2\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Collecting nvidia-nvjitlink-cu12\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Collecting safetensors>=0.4.1\n",
            "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting tokenizers<0.20,>=0.19\n",
            "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3\n",
            "  Using cached huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "Collecting tqdm>=4.27\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Building wheels for collected packages: deepmultilingualpunctuation\n",
            "  Building wheel for deepmultilingualpunctuation (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for deepmultilingualpunctuation: filename=deepmultilingualpunctuation-1.0.1-py3-none-any.whl size=5799 sha256=645f14a4d121bc279d015e0486f2a83bb171a6ccf19a1097dc2e67625e28fc3f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jpbcb04z/wheels/13/14/09/4bbf99cee96e2fbcc7491b5d9087165f0e7f869d276aaa0ec5\n",
            "Successfully built deepmultilingualpunctuation\n",
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, deepmultilingualpunctuation\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.1\n",
            "    Uninstalling urllib3-2.2.1:\n",
            "      Successfully uninstalled urllib3-2.2.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.3\n",
            "    Uninstalling safetensors-0.4.3:\n",
            "      Successfully uninstalled safetensors-0.4.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.15\n",
            "    Uninstalling regex-2024.5.15:\n",
            "      Successfully uninstalled regex-2024.5.15\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.5.0\n",
            "    Uninstalling fsspec-2024.5.0:\n",
            "      Successfully uninstalled fsspec-2024.5.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.14.0\n",
            "    Uninstalling filelock-3.14.0:\n",
            "      Successfully uninstalled filelock-3.14.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.2.2\n",
            "    Uninstalling certifi-2024.2.2:\n",
            "      Successfully uninstalled certifi-2024.2.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.0\n",
            "    Uninstalling huggingface-hub-0.23.0:\n",
            "      Successfully uninstalled huggingface-hub-0.23.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0\n",
            "    Uninstalling torch-2.3.0:\n",
            "      Successfully uninstalled torch-2.3.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "  Attempting uninstall: deepmultilingualpunctuation\n",
            "    Found existing installation: deepmultilingualpunctuation 1.0.1\n",
            "    Uninstalling deepmultilingualpunctuation-1.0.1:\n",
            "      Successfully uninstalled deepmultilingualpunctuation-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "wandb 0.17.0 requires protobuf!=4.21.0,<5,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 5.26.1 which is incompatible.\n",
            "faster-whisper 1.0.0 requires tokenizers<0.16,>=0.13, but you have tokenizers 0.19.1 which is incompatible.\n",
            "demucs 4.1.0a2 requires torchaudio<2.1,>=0.8, but you have torchaudio 2.3.0 which is incompatible.\n",
            "datasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 deepmultilingualpunctuation-1.0.1 filelock-3.14.0 fsspec-2024.5.0 huggingface-hub-0.23.0 idna-3.7 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 packaging-24.0 pyyaml-6.0.1 regex-2024.5.15 requests-2.31.0 safetensors-0.4.3 sympy-1.12 tokenizers-0.19.1 torch-2.3.0 tqdm-4.66.4 transformers-4.40.2 triton-2.3.0 typing-extensions-4.11.0 urllib3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperX.git@78dcfaab51005aa703ee21375f81ed31bc248560\n",
        "!pip install --no-build-isolation nemo_toolkit[asr]==1.23.0\n",
        "!pip install --no-deps git+https://github.com/facebookresearch/demucs#egg=demucs\n",
        "!pip install git+https://github.com/oliverguhr/deepmultilingualpunctuation.git\n",
        "!pip install -U hydra-core inflect editdistance lhotse cython youtokentome webdataset jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YzhncHP0ytbQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/isa/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "torchvision is not available - cannot save figures\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "import json\n",
        "import shutil\n",
        "import whisperx\n",
        "import torch\n",
        "from pydub import AudioSegment\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import re\n",
        "import logging\n",
        "import nltk\n",
        "from whisperx.alignment import DEFAULT_ALIGN_MODELS_HF, DEFAULT_ALIGN_MODELS_TORCH\n",
        "from whisperx.utils import LANGUAGES, TO_LANGUAGE_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsUt3SwyhjD"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Se6Hc7CZygxu"
      },
      "outputs": [],
      "source": [
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "wav2vec2_langs = list(DEFAULT_ALIGN_MODELS_TORCH.keys()) + list(\n",
        "    DEFAULT_ALIGN_MODELS_HF.keys()\n",
        ")\n",
        "\n",
        "whisper_langs = sorted(LANGUAGES.keys()) + sorted(\n",
        "    [k.title() for k in TO_LANGUAGE_CODE.keys()]\n",
        ")\n",
        "\n",
        "\n",
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting, telephonic, or general based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    data_dir = os.path.join(output_dir, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "    config.num_workers = 0  # Workaround for multiprocessing hanging with ipython issue\n",
        "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = (\n",
        "            int(wrd_dict[\"start\"] * 1000),\n",
        "            int(wrd_dict[\"end\"] * 1000),\n",
        "            wrd_dict[\"word\"],\n",
        "        )\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list)\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk or sentence_checker(snt[\"text\"] + \" \" + wrd):\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    previous_speaker = sentences_speaker_mapping[0][\"speaker\"]\n",
        "    f.write(f\"{previous_speaker}: \")\n",
        "\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        speaker = sentence_dict[\"speaker\"]\n",
        "        sentence = sentence_dict[\"text\"]\n",
        "\n",
        "        # If this speaker doesn't match the previous one, start a new paragraph\n",
        "        if speaker != previous_speaker:\n",
        "            f.write(f\"\\n\\n{speaker}: \")\n",
        "            previous_speaker = speaker\n",
        "\n",
        "        # No matter what, write the current sentence\n",
        "        f.write(sentence + \" \")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def find_numeral_symbol_tokens(tokenizer):\n",
        "    numeral_symbol_tokens = [\n",
        "        -1,\n",
        "    ]\n",
        "    for token, token_id in tokenizer.get_vocab().items():\n",
        "        has_numeral_symbol = any(c in \"0123456789%$£\" for c in token)\n",
        "        if has_numeral_symbol:\n",
        "            numeral_symbol_tokens.append(token_id)\n",
        "    return numeral_symbol_tokens\n",
        "\n",
        "\n",
        "def _get_next_start_timestamp(word_timestamps, current_word_index, final_timestamp):\n",
        "    # if current word is the last word\n",
        "    if current_word_index == len(word_timestamps) - 1:\n",
        "        return word_timestamps[current_word_index][\"start\"]\n",
        "\n",
        "    next_word_index = current_word_index + 1\n",
        "    while current_word_index < len(word_timestamps) - 1:\n",
        "        if word_timestamps[next_word_index].get(\"start\") is None:\n",
        "            # if next word doesn't have a start timestamp\n",
        "            # merge it with the current word and delete it\n",
        "            word_timestamps[current_word_index][\"word\"] += (\n",
        "                \" \" + word_timestamps[next_word_index][\"word\"]\n",
        "            )\n",
        "\n",
        "            word_timestamps[next_word_index][\"word\"] = None\n",
        "            next_word_index += 1\n",
        "            if next_word_index == len(word_timestamps):\n",
        "                return final_timestamp\n",
        "\n",
        "        else:\n",
        "            return word_timestamps[next_word_index][\"start\"]\n",
        "\n",
        "\n",
        "def filter_missing_timestamps(\n",
        "    word_timestamps, initial_timestamp=0, final_timestamp=None\n",
        "):\n",
        "    # handle the first and last word\n",
        "    if word_timestamps[0].get(\"start\") is None:\n",
        "        word_timestamps[0][\"start\"] = (\n",
        "            initial_timestamp if initial_timestamp is not None else 0\n",
        "        )\n",
        "        word_timestamps[0][\"end\"] = _get_next_start_timestamp(\n",
        "            word_timestamps, 0, final_timestamp\n",
        "        )\n",
        "\n",
        "    result = [\n",
        "        word_timestamps[0],\n",
        "    ]\n",
        "\n",
        "    for i, ws in enumerate(word_timestamps[1:], start=1):\n",
        "        # if ws doesn't have a start and end\n",
        "        # use the previous end as start and next start as end\n",
        "        if ws.get(\"start\") is None and ws.get(\"word\") is not None:\n",
        "            ws[\"start\"] = word_timestamps[i - 1][\"end\"]\n",
        "            ws[\"end\"] = _get_next_start_timestamp(word_timestamps, i, final_timestamp)\n",
        "\n",
        "        if ws[\"word\"] is not None:\n",
        "            result.append(ws)\n",
        "    return result\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n",
        "\n",
        "\n",
        "def process_language_arg(language: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Process the language argument to make sure it's valid and convert language names to language codes.\n",
        "    \"\"\"\n",
        "    if language is not None:\n",
        "        language = language.lower()\n",
        "    if language not in LANGUAGES:\n",
        "        if language in TO_LANGUAGE_CODE:\n",
        "            language = TO_LANGUAGE_CODE[language]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "    if model_name.endswith(\".en\") and language != \"en\":\n",
        "        if language is not None:\n",
        "            logging.warning(\n",
        "                f\"{model_name} is an English-only model but received '{language}'; using English instead.\"\n",
        "            )\n",
        "        language = \"en\"\n",
        "    return language\n",
        "\n",
        "\n",
        "def transcribe(\n",
        "    audio_file: str,\n",
        "    language: str,\n",
        "    model_name: str,\n",
        "    compute_dtype: str,\n",
        "    suppress_numerals: bool,\n",
        "    device: str,\n",
        "):\n",
        "    from faster_whisper import WhisperModel\n",
        "    from helpers import find_numeral_symbol_tokens, wav2vec2_langs\n",
        "\n",
        "    # Faster Whisper non-batched\n",
        "    # Run on GPU with FP16\n",
        "    whisper_model = WhisperModel(model_name, device=device, compute_type=compute_dtype)\n",
        "\n",
        "    # or run on GPU with INT8\n",
        "    # model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "    # or run on CPU with INT8\n",
        "    # model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
        "\n",
        "    if suppress_numerals:\n",
        "        numeral_symbol_tokens = find_numeral_symbol_tokens(whisper_model.hf_tokenizer)\n",
        "    else:\n",
        "        numeral_symbol_tokens = None\n",
        "\n",
        "    if language is not None and language in wav2vec2_langs:\n",
        "        word_timestamps = False\n",
        "    else:\n",
        "        word_timestamps = True\n",
        "\n",
        "    segments, info = whisper_model.transcribe(\n",
        "        audio_file,\n",
        "        language=language,\n",
        "        beam_size=5,\n",
        "        word_timestamps=word_timestamps,  # TODO: disable this if the language is supported by wav2vec2\n",
        "        suppress_tokens=numeral_symbol_tokens,\n",
        "        vad_filter=True,\n",
        "    )\n",
        "    whisper_results = []\n",
        "    for segment in segments:\n",
        "        whisper_results.append(segment._asdict())\n",
        "    # clear gpu vram\n",
        "    del whisper_model\n",
        "    torch.cuda.empty_cache()\n",
        "    return whisper_results, language\n",
        "\n",
        "\n",
        "def transcribe_batched(\n",
        "    audio_file: str,\n",
        "    language: str,\n",
        "    batch_size: int,\n",
        "    model_name: str,\n",
        "    compute_dtype: str,\n",
        "    suppress_numerals: bool,\n",
        "    device: str,\n",
        "):\n",
        "    import whisperx\n",
        "\n",
        "    # Faster Whisper batched\n",
        "    whisper_model = whisperx.load_model(\n",
        "        model_name,\n",
        "        device,\n",
        "        compute_type=compute_dtype,\n",
        "        asr_options={\"suppress_numerals\": suppress_numerals},\n",
        "    )\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = whisper_model.transcribe(audio, language=language, batch_size=batch_size)\n",
        "    del whisper_model\n",
        "    torch.cuda.empty_cache()\n",
        "    return result[\"segments\"], result[\"language\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qWQb--1Xcw"
      },
      "source": [
        "# Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ONlFrSnD0FOp"
      },
      "outputs": [],
      "source": [
        "# Name of the audio file\n",
        "audio_path = \"test.wav\"\n",
        "\n",
        "# Whether to enable music removal from speech, helps increase diarization quality but uses alot of ram\n",
        "enable_stemming = False\n",
        "\n",
        "# (choose from 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large')\n",
        "whisper_model_name = \"large-v3\"\n",
        "\n",
        "# replaces numerical digits with their pronounciation, increases diarization accuracy\n",
        "suppress_numerals = True\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "language = None  # autodetect language\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-cY1ZEy2KVI"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZS4xXmE2NGP"
      },
      "source": [
        "## Separating music from speech using Demucs\n",
        "\n",
        "---\n",
        "\n",
        "By isolating the vocals from the rest of the audio, it becomes easier to identify and track individual speakers based on the spectral and temporal characteristics of their speech signals. Source separation is just one of many techniques that can be used as a preprocessing step to help improve the accuracy and reliability of the overall diarization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HKcgQUrAzsJZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: pt (1.00) in first 30s of audio...\n",
            "Suppressing numeral and symbol tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-portuguese were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-portuguese and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:15:46 msdd_models:1092] Loading pretrained diar_msdd_telephonic model from NGC\n",
            "[NeMo I 2024-05-16 18:15:46 cloud:58] Found existing object /home/isa/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2024-05-16 18:15:46 cloud:64] Re-using file from: /home/isa/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
            "[NeMo I 2024-05-16 18:15:46 common:924] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-05-16 18:15:48 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: true\n",
            "    \n",
            "[NeMo W 2024-05-16 18:15:48 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2024-05-16 18:15:48 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    seq_eval_mode: false\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:15:48 features:289] PADDING: 16\n",
            "[NeMo I 2024-05-16 18:15:48 features:289] PADDING: 16\n",
            "[NeMo I 2024-05-16 18:15:48 save_restore_connector:249] Model EncDecDiarLabelModel was successfully restored from /home/isa/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2024-05-16 18:15:48 features:289] PADDING: 16\n",
            "[NeMo I 2024-05-16 18:15:49 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
            "[NeMo I 2024-05-16 18:15:49 cloud:58] Found existing object /home/isa/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2024-05-16 18:15:49 cloud:64] Re-using file from: /home/isa/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
            "[NeMo I 2024-05-16 18:15:49 common:924] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-05-16 18:15:49 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    tarred_shard_strategy: scatter\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 0.5\n",
            "        min_shift_ms: -10.0\n",
            "        max_shift_ms: 10.0\n",
            "      white_noise:\n",
            "        prob: 0.5\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "        norm: true\n",
            "      noise:\n",
            "        prob: 0.5\n",
            "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
            "        min_snr_db: 0\n",
            "        max_snr_db: 30\n",
            "        max_gain_db: 300.0\n",
            "        norm: true\n",
            "      gain:\n",
            "        prob: 0.5\n",
            "        min_gain_dbfs: -10.0\n",
            "        max_gain_dbfs: 10.0\n",
            "        norm: true\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2024-05-16 18:15:49 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2024-05-16 18:15:49 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:15:49 features:289] PADDING: 16\n",
            "[NeMo I 2024-05-16 18:15:49 save_restore_connector:249] Model EncDecClassificationModel was successfully restored from /home/isa/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2024-05-16 18:15:49 msdd_models:864] Multiscale Weights: [1, 1, 1, 1, 1]\n",
            "[NeMo I 2024-05-16 18:15:49 msdd_models:865] Clustering Parameters: {\n",
            "        \"oracle_num_speakers\": false,\n",
            "        \"max_num_speakers\": 8,\n",
            "        \"enhanced_count_thres\": 80,\n",
            "        \"max_rp_threshold\": 0.25,\n",
            "        \"sparse_search_volume\": 30,\n",
            "        \"maj_vote_spk_count\": false,\n",
            "        \"chunk_cluster_count\": 50,\n",
            "        \"embeddings_per_chunk\": 10000\n",
            "    }\n",
            "[NeMo I 2024-05-16 18:15:49 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2024-05-16 18:15:49 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "splitting manifest: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:15:50 classification_models:273] Perform streaming frame-level VAD\n",
            "[NeMo I 2024-05-16 18:15:50 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:15:50 collections:446] Dataset loaded with 20 items, total duration of  0.27 hours.\n",
            "[NeMo I 2024-05-16 18:15:50 collections:448] # 20 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "vad: 100%|██████████| 20/20 [00:07<00:00,  2.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:15:57 clustering_diarizer:250] Generating predictions with overlapping input segments\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                                                               "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:04 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "creating speech segments: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:05 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale0.json\n",
            "[NeMo I 2024-05-16 18:16:05 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-05-16 18:16:05 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:16:05 collections:446] Dataset loaded with 1051 items, total duration of  0.41 hours.\n",
            "[NeMo I 2024-05-16 18:16:05 collections:448] # 1051 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[1/5] extract embeddings: 100%|██████████| 17/17 [00:02<00:00,  7.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:07 clustering_diarizer:389] Saved embedding files to /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-05-16 18:16:07 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale1.json\n",
            "[NeMo I 2024-05-16 18:16:07 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-05-16 18:16:07 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:16:07 collections:446] Dataset loaded with 1277 items, total duration of  0.42 hours.\n",
            "[NeMo I 2024-05-16 18:16:07 collections:448] # 1277 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/5] extract embeddings: 100%|██████████| 20/20 [00:02<00:00,  9.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:10 clustering_diarizer:389] Saved embedding files to /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-05-16 18:16:10 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale2.json\n",
            "[NeMo I 2024-05-16 18:16:10 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-05-16 18:16:10 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:16:10 collections:446] Dataset loaded with 1607 items, total duration of  0.43 hours.\n",
            "[NeMo I 2024-05-16 18:16:10 collections:448] # 1607 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[3/5] extract embeddings: 100%|██████████| 26/26 [00:02<00:00,  9.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:12 clustering_diarizer:389] Saved embedding files to /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-05-16 18:16:12 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale3.json\n",
            "[NeMo I 2024-05-16 18:16:12 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-05-16 18:16:12 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:16:12 collections:446] Dataset loaded with 2178 items, total duration of  0.44 hours.\n",
            "[NeMo I 2024-05-16 18:16:12 collections:448] # 2178 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[4/5] extract embeddings: 100%|██████████| 35/35 [00:03<00:00, 10.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:16 clustering_diarizer:389] Saved embedding files to /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-05-16 18:16:16 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale4.json\n",
            "[NeMo I 2024-05-16 18:16:16 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-05-16 18:16:16 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-05-16 18:16:16 collections:446] Dataset loaded with 3320 items, total duration of  0.45 hours.\n",
            "[NeMo I 2024-05-16 18:16:16 collections:448] # 3320 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[5/5] extract embeddings: 100%|██████████| 52/52 [00:04<00:00, 12.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:20 clustering_diarizer:389] Saved embedding files to /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "clustering: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:21 clustering_diarizer:464] Outputs are saved in /home/isa/linus/llm-jus/temp_outputs directory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[NeMo W 2024-05-16 18:16:21 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:21 msdd_models:960] Loading embedding pickle file of scale:0 at /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
            "[NeMo I 2024-05-16 18:16:21 msdd_models:960] Loading embedding pickle file of scale:1 at /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
            "[NeMo I 2024-05-16 18:16:21 msdd_models:960] Loading embedding pickle file of scale:2 at /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
            "[NeMo I 2024-05-16 18:16:21 msdd_models:960] Loading embedding pickle file of scale:3 at /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
            "[NeMo I 2024-05-16 18:16:21 msdd_models:960] Loading embedding pickle file of scale:4 at /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
            "[NeMo I 2024-05-16 18:16:21 msdd_models:938] Loading cluster label file from /home/isa/linus/llm-jus/temp_outputs/speaker_outputs/subsegments_scale4_cluster.label\n",
            "[NeMo I 2024-05-16 18:16:22 collections:761] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2024-05-16 18:16:22 collections:764] Total 1 session files loaded accounting to # 1 audio clips\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:22 msdd_models:1403]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
            "[NeMo I 2024-05-16 18:16:22 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2024-05-16 18:16:22 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[NeMo W 2024-05-16 18:16:22 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:22 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-05-16 18:16:22 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:22 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-05-16 18:16:22 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-05-16 18:16:22 msdd_models:1431]   \n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ],
      "source": [
        "if enable_stemming:\n",
        "    # Isolate vocals from the rest of the audio\n",
        "\n",
        "    return_code = os.system(\n",
        "        f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{audio_path}\" -o \"temp_outputs\"'\n",
        "    )\n",
        "\n",
        "    if return_code != 0:\n",
        "        logging.warning(\"Source splitting failed, using original audio file.\")\n",
        "        vocal_target = audio_path\n",
        "    else:\n",
        "        vocal_target = os.path.join(\n",
        "            \"temp_outputs\",\n",
        "            \"htdemucs\",\n",
        "            os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            \"vocals.wav\",\n",
        "        )\n",
        "else:\n",
        "    vocal_target = audio_path\n",
        "\n",
        "\n",
        "compute_type = \"float16\"\n",
        "# or run on GPU with INT8\n",
        "# compute_type = \"int8_float16\"\n",
        "# or run on CPU with INT8\n",
        "# compute_type = \"int8\"\n",
        "\n",
        "if batch_size != 0:\n",
        "    whisper_results, language = transcribe_batched(\n",
        "        vocal_target,\n",
        "        language,\n",
        "        batch_size,\n",
        "        whisper_model_name,\n",
        "        compute_type,\n",
        "        suppress_numerals,\n",
        "        device,\n",
        "    )\n",
        "else:\n",
        "    whisper_results, language = transcribe(\n",
        "        vocal_target,\n",
        "        language,\n",
        "        whisper_model_name,\n",
        "        compute_type,\n",
        "        suppress_numerals,\n",
        "        device,\n",
        "    )\n",
        "\n",
        "\n",
        "if language in wav2vec2_langs:\n",
        "    device = \"cuda\"\n",
        "    alignment_model, metadata = whisperx.load_align_model(\n",
        "        language_code=language, device=device\n",
        "    )\n",
        "    result_aligned = whisperx.align(\n",
        "        whisper_results, alignment_model, metadata, vocal_target, device\n",
        "    )\n",
        "    word_timestamps = filter_missing_timestamps(\n",
        "        result_aligned[\"word_segments\"],\n",
        "        initial_timestamp=whisper_results[0].get(\"start\"),\n",
        "        final_timestamp=whisper_results[-1].get(\"end\"),\n",
        "    )\n",
        "\n",
        "    # clear gpu vram\n",
        "    del alignment_model\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    assert batch_size == 0, (  # TODO: add a better check for word timestamps existence\n",
        "        f\"Unsupported language: {language}, use --batch_size to 0\"\n",
        "        \" to generate word timestamps using whisper directly and fix this error.\"\n",
        "    )\n",
        "    word_timestamps = []\n",
        "    for segment in whisper_results:\n",
        "        for word in segment[\"words\"]:\n",
        "            word_timestamps.append({\"word\": word[2], \"start\": word[0], \"end\": word[1]})        \n",
        "\n",
        "sound = AudioSegment.from_file(vocal_target).set_channels(1)\n",
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "sound.export(os.path.join(temp_path, \"mono_file.wav\"), format=\"wav\")            \n",
        "\n",
        "# Initialize NeMo MSDD diarization model\n",
        "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "speaker_ts = []\n",
        "with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(\" \")\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
        "\n",
        "if language in punct_model_langs:\n",
        "    # restoring punctuation in the transcript to help realign the sentences\n",
        "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
        "\n",
        "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
        "\n",
        "    labled_words = punct_model.predict(words_list)\n",
        "\n",
        "    ending_puncts = \".?!\"\n",
        "    model_puncts = \".,;:!?\"\n",
        "\n",
        "    # We don't want to punctuate U.S.A. with a period. Right?\n",
        "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "        word = word_dict[\"word\"]\n",
        "        if (\n",
        "            word\n",
        "            and labeled_tuple[1] in ending_puncts\n",
        "            and (word[-1] not in model_puncts or is_acronym(word))\n",
        "        ):\n",
        "            word += labeled_tuple[1]\n",
        "            if word.endswith(\"..\"):\n",
        "                word = word.rstrip(\".\")\n",
        "            word_dict[\"word\"] = word\n",
        "\n",
        "else:\n",
        "    logging.warning(\n",
        "        f\"Punctuation restoration is not available for {language} language. Using the original punctuation.\"\n",
        "    )\n",
        "\n",
        "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
        "\n",
        "\n",
        "with open(f\"{os.path.splitext(audio_path)[0]}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
        "    get_speaker_aware_transcript(ssm, f)\n",
        "\n",
        "with open(f\"{os.path.splitext(audio_path)[0]}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n",
        "    write_srt(ssm, srt)\n",
        "\n",
        "cleanup(temp_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eCmjcOc9yEtQ",
        "jbsUt3SwyhjD"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
